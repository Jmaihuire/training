{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â # New architecture for handwritting\n",
    "     - MNIST sequence\n",
    "     - Datasets\n",
    "     - Dilated convolutions\n",
    "     - CTC\n",
    "     \n",
    "     \n",
    "# Next steps:\n",
    "    - change each conv 1d by a module whit batch normalization and convs 1x1\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import math\n",
    "import logging\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 10) \n",
    "\n",
    "data_path = '/home/ubuntu/data/oxford_syntetic_text/mnt/ramdisk/max/90kDICT32px'\n",
    "\n",
    "experiment_dir ='/home/ubuntu/data/oxford_syntetic_text/models/test03'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_to_sparse(dense_tensor, out_type):\n",
    "    indices = tf.where(tf.not_equal(dense_tensor, tf.constant(-1, dense_tensor.dtype)))\n",
    "    values = tf.gather_nd(dense_tensor, indices)\n",
    "    shape = tf.shape(dense_tensor, out_type=out_type)\n",
    "    return tf.SparseTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def decode_word(l, decoder_dict, blank_code=-1):\n",
    "        return ''.join([decoder_dict[x] for x in l if x!=blank_code])\n",
    "    \n",
    "    \n",
    "\n",
    "def evaluate_wer(pred, real):\n",
    "    wer = 0\n",
    "    for p, r in zip(pred, real):\n",
    "        try:\n",
    "            if p != r:\n",
    "                wer +=1\n",
    "        except:\n",
    "            print(p, r)\n",
    "    wer = wer / len(pred)\n",
    "    return wer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images train:  7224612\n",
      "Images valid:  802734\n",
      "Images test:   891927\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(os.path.join(data_path,'annotation_train.txt'), delimiter=' ', names=['file', 'n'])\n",
    "df_val   = pd.read_csv(os.path.join(data_path,'annotation_val.txt'  ), delimiter=' ', names=['file', 'n'])\n",
    "df_test  = pd.read_csv(os.path.join(data_path,'annotation_test.txt' ), delimiter=' ', names=['file', 'n'])\n",
    "print('Images train: ', df_train.shape[0])\n",
    "print('Images valid: ', df_val.shape[0])\n",
    "print('Images test:  ', df_test.shape[0])\n",
    "\n",
    "        \n",
    "# Cuda devices\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "gpu_options = tf.GPUOptions(allow_growth = True)\n",
    "    \n",
    "\n",
    "# Decoder dict and num_classes\n",
    "char_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E',\n",
    "     'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U',\n",
    "     'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',\n",
    "     'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']    \n",
    "encoder_dict = {}\n",
    "for i, c in enumerate(char_list):\n",
    "    encoder_dict[c] = i\n",
    "decoder_dict = {}\n",
    "for e in encoder_dict:\n",
    "    decoder_dict[encoder_dict[e]]=e\n",
    "num_classes = len(decoder_dict) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_image(img_file, x_size=192, y_size=48, x_blanks_ini=0):\n",
    "    \n",
    "    im = Image.open(img_file)\n",
    "    if np.max(im)>0:\n",
    "        x, y = im.size\n",
    "        factor = y_size/y\n",
    "        new_x = min( max(1, int(factor*x)), x_size-x_blanks_ini)\n",
    "        if len(np.array(im.resize((new_x, y_size))).shape) == 3:\n",
    "            img = np.array(im.resize((new_x, y_size)))[:,:,0]\n",
    "        else:\n",
    "            img = np.array(im.resize((new_x, y_size)))\n",
    "        img_adjusted = np.concatenate([np.zeros((y_size, x_blanks_ini)), img], axis=1)\n",
    "        new_x_size = img_adjusted.shape[1]\n",
    "        if new_x_size < x_size:\n",
    "            img_adjusted = np.concatenate([img_adjusted, np.zeros((y_size, x_size-new_x_size))], axis=1)\n",
    "\n",
    "        if np.max(img_adjusted)>0:\n",
    "            return img_adjusted\n",
    "        else:\n",
    "            return []\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def read_word_image(f, target_c, x_size=192, y_size=48, target_max_size=19):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # adjust and resize to the final size\n",
    "    img_adjusted = adjust_image(f, x_size=x_size, y_size=y_size)\n",
    "    \n",
    "    if img_adjusted != []:\n",
    "\n",
    "        #Calculate image_len\n",
    "        image_len = np.max(np.nonzero(np.max(img_adjusted, axis=0))) \n",
    "\n",
    "        # Target\n",
    "        target_ini = [encoder_dict[k] for k in target_c] # encode to \n",
    "        if len(target_ini)>target_max_size: # Pendiente de resolver mejor\n",
    "            target_ini = target_ini[:target_max_size]\n",
    "        target_len = len(target_ini)\n",
    "        target = np.ones([target_max_size], dtype=np.uint8)*(-1)\n",
    "        target[:target_len] = target_ini\n",
    "                        \n",
    "        return img_adjusted/255, list(target), image_len, target_len\n",
    "    \n",
    "    else:\n",
    "        return [], None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/tf18/lib/python3.5/site-packages/ipykernel/__main__.py:32: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 48, 192, 1)\n"
     ]
    }
   ],
   "source": [
    "def data_generator(file_list, path_files=data_path, batch_size=16, max_files=0):\n",
    "    if max_files==0: # all files\n",
    "        num_batches = len(file_list)//batch_size\n",
    "    else:\n",
    "        num_batches = min(max_files//batch_size, len(file_list)//batch_size)\n",
    "    n = 0    \n",
    "    \n",
    "    # Shuffle files\n",
    "    np.random.shuffle(file_list)\n",
    "    \n",
    "    for j in range(num_batches):\n",
    "        images_batch = []\n",
    "        images_len_batch = []\n",
    "        target_batch = []\n",
    "        target_len_batch = []\n",
    "        for i in range(batch_size):\n",
    "            f = os.path.join(path_files, file_list[n])\n",
    "            target_c = f.split('/')[-1].split('.')[0].split('_')[1] \n",
    "            img, t, img_l, t_l = read_word_image(os.path.join(path_files, f), target_c, x_size=192, y_size=48, target_max_size=19)\n",
    "            img = np.reshape(img, (48, 192, 1))\n",
    "            images_batch += [img]\n",
    "            images_len_batch += [img_l]\n",
    "            target_batch += [t]\n",
    "            target_len_batch += [t_l]\n",
    "            n += 1\n",
    "            \n",
    "        yield np.array(images_batch), images_len_batch, target_batch, target_len_batch\n",
    "            \n",
    "# test\n",
    "trn_generator = data_generator(list(df_test.file), path_files=data_path, batch_size=16, max_files=100) \n",
    "images_b, images_len_b, target_b, target_len_b = next(trn_generator)\n",
    "print(images_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created!\n"
     ]
    }
   ],
   "source": [
    "# Size of \n",
    "filters_list = [512,512,512,512,512]\n",
    "kernels_list = [3,3,3,3,3]\n",
    "dilations_list = [1,1,2,4,8,16]\n",
    "\n",
    "                        \n",
    "if 1:\n",
    "    # Model\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        #with tf.device('/cpu:0'): # Check how to put this on CPU\n",
    "        if 1:\n",
    "\n",
    "            #Placeholders\n",
    "            with tf.name_scope('inputs') as scope:\n",
    "\n",
    "                # List of TFRecod filenames (for train, valid and test)\n",
    "                images_batch_ph = tf.placeholder(tf.float32, shape=[None, 48, 192, 1], name='images_batch_ph')\n",
    "                image_len_ph = tf.placeholder(tf.int32, shape=[None], name='image_len_ph')\n",
    "                labels_batch_ph = tf.placeholder(tf.int32, shape=[None, 19], name='labels_batch_ph')\n",
    "                labels_len_batch_ph = tf.placeholder(tf.int32, shape=[None], name='labels_len_batch_ph')\n",
    "                \n",
    "                # Convert target to sparse\n",
    "                target = tf.cast(dense_to_sparse(labels_batch_ph, tf.int64), tf.int32)\n",
    "                target_len = tf.cast(labels_len_batch_ph, tf.int32)\n",
    "\n",
    "                # Dropout parameter\n",
    "                #keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "        if 1:\n",
    "            with tf.name_scope('model') as scope:\n",
    "\n",
    "\n",
    "                # First 2 2D convolution of 3x3with 10 filters\n",
    "                conv2d = tf.layers.conv2d(images_batch_ph, 20, 5, padding='SAME')\n",
    "                conv2d = tf.layers.conv2d(conv2d, 20, 5, padding='SAME')\n",
    "                conv2d = tf.layers.max_pooling2d(conv2d, 2, 2)\n",
    "\n",
    "                conv2d = tf.layers.conv2d(conv2d, 50, 5, padding='SAME')\n",
    "                conv2d = tf.layers.conv2d(conv2d, 50, 5, padding='SAME')\n",
    "                conv2d = tf.layers.max_pooling2d(conv2d, 2, 2)\n",
    "                \n",
    "                \n",
    "                # convert to list by filters\n",
    "                conv2d_unstack = tf.unstack(conv2d, axis=-1)\n",
    "                conv1_list=[]\n",
    "                for conv2_filter in conv2d_unstack: # for each filter of shape 28x140 - 7X35 ...\n",
    "                    conv2_filter_transpose = tf.transpose(conv2_filter, (0, 2, 1)) #  convert to 140x28 - 35X7\n",
    "                    conv1_list += [conv2_filter_transpose] \n",
    "                # Concatenate\n",
    "                convf = tf.concat(conv1_list, axis=-1) # out of 140x200 \n",
    "                \n",
    "                # Final 1d convolutions stacked\n",
    "                for filters, kernel, dilation in zip(filters_list, kernels_list, dilations_list):\n",
    "                    convf_out = tf.layers.conv1d(convf, filters=filters, kernel_size=[kernel], activation=tf.nn.relu, padding='SAME', dilation_rate=[dilation])\n",
    "                    convf = tf.concat([convf_out, convf], axis=-1) # Residual connections\n",
    "    \n",
    "                # Reshape vector to classes+1 with dense layers\n",
    "                logits_input = tf.layers.conv1d(convf, filters=num_classes, kernel_size=[1], activation=tf.nn.relu, padding='SAME')\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "            # Create logits\n",
    "            with tf.name_scope(\"Logit\") as scope:\n",
    "                logits = tf.transpose(logits_input, (1, 0, 2), name='logits') #Time major [t, b, NClasses+1], for CTC\n",
    "                #variable_summaries(logits, 'logits')\n",
    "\n",
    "            # Create CTC loss\n",
    "            with tf.name_scope(\"loss\") as scope:\n",
    "                sequence_len = tf.ones_like(image_len_ph)*tf.constant(35)\n",
    "                sequence_len = tf.cast(sequence_len, tf.int32)\n",
    "                loss = tf.nn.ctc_loss(target, logits, sequence_len, ignore_longer_outputs_than_inputs=True)    \n",
    "                cost = tf.reduce_mean(loss, name='cost')\n",
    "                cost_summary = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "\n",
    "            #Optimizer\n",
    "            with tf.name_scope(\"train\") as scope:\n",
    "                global_step = tf.Variable(0, trainable=False)\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate=0.0001, momentum=0.97)\n",
    "                \n",
    "                gvs = optimizer.compute_gradients(cost)\n",
    "                #for i,t in enumerate(gvs):\n",
    "                #    logger.info('gradients: %s - %s', i, t)\n",
    "                #    variable_summaries(t, 'grad'+str(i))\n",
    "                #capped_gvs = gvs\n",
    "                capped_gvs = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in gvs]\n",
    "                train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
    "\n",
    "\n",
    "\n",
    "            # decode CTC\n",
    "            with tf.name_scope(\"predict\") as scope:\n",
    "                decoded, log_prob = tf.nn.ctc_beam_search_decoder(logits, sequence_len, merge_repeated=False)\n",
    "\n",
    "                prediction = tf.cast(decoded[0], tf.int32, name='prediction')\n",
    "                dense_prediction = tf.sparse_to_dense(prediction.indices, prediction.dense_shape,\n",
    "                                                      prediction.values, default_value=-1)\n",
    "\n",
    "\n",
    "            # Accuracy --> Levensteing distance: CER/num_chars \n",
    "            with tf.name_scope(\"accuracy\") as scope:\n",
    "                accuracy = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), target), name='acuracy')\n",
    "                accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "            # Summaries\n",
    "            summaries_dir = os.path.join(experiment_dir)\n",
    "            merged = tf.summary.merge_all()\n",
    "\n",
    "            # Saver\n",
    "            tf.add_to_collection('images_batch_ph', images_batch_ph)\n",
    "            tf.add_to_collection('image_len_ph', image_len_ph)\n",
    "            tf.add_to_collection('labels_batch_ph', labels_batch_ph)\n",
    "            tf.add_to_collection('labels_len_batch_ph', labels_len_batch_ph)\n",
    "            tf.add_to_collection('logits', logits)\n",
    "            tf.add_to_collection('dense_prediction', dense_prediction)\n",
    "\n",
    "            saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "\n",
    "\n",
    "    print('Model created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(epoch, decoder_dict):\n",
    "    '''\n",
    "    '''\n",
    "    step=1\n",
    "    cost_l = []\n",
    "    acc_l = []\n",
    "    train_generator = data_generator(list(df_train.file)[:150000])\n",
    "    for img, img_len, t, t_len in train_generator:\n",
    "        _, ce, acc = sess.run([train_op, cost, accuracy], \n",
    "                              feed_dict={images_batch_ph: img, image_len_ph: img_len,\n",
    "                                        labels_batch_ph: t, labels_len_batch_ph: t_len})\n",
    "        cost_l += [ce]\n",
    "        acc_l += [acc]\n",
    "        step += 1\n",
    "        if step%500 == 0:\n",
    "            # Sumaries train\n",
    "            summary_str, pred = sess.run([merged, dense_prediction], \n",
    "                                  feed_dict={images_batch_ph: img, image_len_ph: img_len,\n",
    "                                        labels_batch_ph: t, labels_len_batch_ph: t_len}) \n",
    "            train_writer.add_summary(summary_str, epoch)\n",
    "            pred = [decode_word(w, decoder_dict) for w in pred]\n",
    "            real = [decode_word(w, decoder_dict) for w in t]\n",
    "            wer = evaluate_wer(pred, real)\n",
    "            \n",
    "            print('TRAIN - Epoch:', epoch,\n",
    "                  ' - Step:', step,\n",
    "                  ' - Cost:', np.mean(cost_l),\n",
    "                  ' - CER:', np.mean(acc_l),\n",
    "                  ' - WER (step):', wer)\n",
    "            cost_l = [] #Reset\n",
    "            acc_l = []  \n",
    "            \n",
    "            print('\\nTrain examples pred vs real:')\n",
    "            for i in range(7):\n",
    "                print(pred[i], ' - ', real[i])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(decoder_dict):\n",
    "    '''\n",
    "    '''\n",
    "    step=1\n",
    "    cost_l = []\n",
    "    cer_l = []\n",
    "    wer_l = []\n",
    "    pred_l = []\n",
    "    real_l = []\n",
    "    val_generator = data_generator(list(df_val.file)[:2048])\n",
    "    for img, img_len, t, t_len in val_generator:\n",
    "        ce, acc, pred = sess.run([cost, accuracy, dense_prediction], \n",
    "                              feed_dict={images_batch_ph: img, image_len_ph: img_len,\n",
    "                                        labels_batch_ph: t, labels_len_batch_ph: t_len})\n",
    "        pred = [decode_word(w, decoder_dict) for w in pred]\n",
    "        real = [decode_word(w, decoder_dict) for w in t]\n",
    "        cost_l += [ce]\n",
    "        cer_l += [acc]\n",
    "        pred_l += pred\n",
    "        real_l += real\n",
    "        step +=1\n",
    "        \n",
    "    # Sumaries eval\n",
    "    summary_str = sess.run(merged, feed_dict={images_batch_ph: img, image_len_ph: img_len,\n",
    "                                        labels_batch_ph: t, labels_len_batch_ph: t_len}) \n",
    "    test_writer.add_summary(summary_str, epoch)\n",
    "    \n",
    "    wer = evaluate_wer(pred_l, real_l)\n",
    "            \n",
    "    print('TEST - Cost:', np.mean(cost_l),\n",
    "            ' - CER:', np.mean(cer_l),\n",
    "            ' - WER:', wer)\n",
    "    print('\\nTest examples pred vs real:')\n",
    "    for i in range(10):\n",
    "        print(pred_l[i], ' - ', real_l[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return cost_l, cer_l, wer_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vars initialized!\n",
      "Training epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/tf18/lib/python3.5/site-packages/ipykernel/__main__.py:32: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN - Epoch: 1  - Step: 500  - Cost: 35.6782  - CER: 0.9600266  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "e  -  goatee\n",
      "e  -  finn\n",
      "e  -  Resharpened\n",
      "e  -  STUPENDOUS\n",
      "e  -  frisbee\n",
      "e  -  cps\n",
      "ae  -  Liberties\n",
      "TRAIN - Epoch: 1  - Step: 1000  - Cost: 31.081186  - CER: 0.952635  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "s  -  STILLED\n",
      "Ce  -  SERAGLIO\n",
      "s  -  AUTOPILOT\n",
      "s  -  Permanently\n",
      "S  -  Biblical\n",
      "s  -  PARASITICALLY\n",
      "s  -  Vertigo\n",
      "TRAIN - Epoch: 1  - Step: 1500  - Cost: 30.77977  - CER: 0.93838406  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "Ce  -  Incompetents\n",
      "ee  -  misplaced\n",
      "Se  -  Pronuclear\n",
      "ee  -  Particularity\n",
      "Re  -  Equestriennes\n",
      "Se  -  REATTAINS\n",
      "e  -  GARMON\n",
      "TRAIN - Epoch: 1  - Step: 2000  - Cost: 30.224266  - CER: 0.94111973  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "e  -  HALO\n",
      "E  -  SE\n",
      "a  -  Centerboards\n",
      "E  -  PUFFERS\n",
      "e  -  defacement\n",
      "a  -  Majorly\n",
      "a  -  Tenpins\n",
      "TRAIN - Epoch: 1  - Step: 2500  - Cost: 29.862984  - CER: 0.9379568  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "e  -  bunchiest\n",
      "a  -  egocentric\n",
      "ae  -  Qualmish\n",
      "E  -  GARLICKY\n",
      "EiTNT  -  YTTERBIUM\n",
      "E  -  laureateship\n",
      "e  -  Digitizing\n",
      "TRAIN - Epoch: 1  - Step: 3000  - Cost: 29.178844  - CER: 0.93120885  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "A  -  SADHU\n",
      "i  -  shithead\n",
      "uo  -  Humanize\n",
      "a  -  compotes\n",
      "o  -  COGENTLY\n",
      "o  -  Troyes\n",
      "E  -  CAP\n",
      "TRAIN - Epoch: 1  - Step: 3500  - Cost: 28.45084  - CER: 0.9163275  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "i  -  Ted\n",
      "iee  -  sleetier\n",
      "oee  -  mussorgsky\n",
      "En  -  LINESMEN\n",
      "Ai  -  attained\n",
      "o  -  cassocks\n",
      "an  -  INVERTING\n",
      "TRAIN - Epoch: 1  - Step: 4000  - Cost: 27.418177  - CER: 0.8953489  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "E  -  romeos\n",
      "ae  -  aspirator\n",
      "A  -  AGENCY\n",
      "o  -  sums\n",
      "an  -  CONTRAFLOWS\n",
      "E  -  TABBIES\n",
      "e  -  MADISON\n",
      "TRAIN - Epoch: 1  - Step: 4500  - Cost: 26.84177  - CER: 0.8626128  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "ans  -  STADIUMS\n",
      "u  -  burn\n",
      "oae  -  Draughtboards\n",
      "TN  -  TEEING\n",
      "ee  -  Igneous\n",
      "Caes  -  Captures\n",
      "itls  -  Differential\n",
      "TRAIN - Epoch: 1  - Step: 5000  - Cost: 25.327375  - CER: 0.8158164  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "AIIT  -  PARTURITION\n",
      "Ceees  -  ICEBERGS\n",
      "st  -  sb\n",
      "Iti  -  agoraphobia\n",
      "sacE  -  Stupefy\n",
      "at  -  borer\n",
      "apied  -  Vaporized\n",
      "TRAIN - Epoch: 1  - Step: 5500  - Cost: 23.917294  - CER: 0.7606441  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "oS  -  raconteurs\n",
      "LO  -  Mylar\n",
      "Caeei  -  Nonbelievers\n",
      "SET  -  Swamp\n",
      "AEES  -  WARNER\n",
      "IRPES  -  VITRIFIES\n",
      "AS  -  BANKNOTE\n",
      "TRAIN - Epoch: 1  - Step: 6000  - Cost: 22.244884  - CER: 0.70634025  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "IUronreS  -  backwardness\n",
      "rsle  -  Insole\n",
      "Elrerh  -  Wolfish\n",
      "cianmantly  -  adamantly\n",
      "SLETEs  -  SUBJUGATES\n",
      "HleAS  -  theorems\n",
      "Lruge  -  Luge\n",
      "TRAIN - Epoch: 1  - Step: 6500  - Cost: 20.114738  - CER: 0.62552375  - WER (step): 1.0\n",
      "\n",
      "Train examples pred vs real:\n",
      "Syioaas  -  STATIONERY\n",
      "Wn  -  Vdt\n",
      "MuNsEIaTs  -  AMUSEMENTS\n",
      "Pr  -  Prays\n",
      "dbanetnt  -  debarment\n",
      "UNESs  -  WILINESS\n",
      "EaacGs  -  SPRUCES\n",
      "TRAIN - Epoch: 1  - Step: 7000  - Cost: 18.069944  - CER: 0.5533528  - WER (step): 0.875\n",
      "\n",
      "Train examples pred vs real:\n",
      "CNct  -  CONDIGN\n",
      "UOSpecig  -  unspecific\n",
      "Slumlard  -  slumlord\n",
      "FiD  -  OPINION\n",
      "Drong  -  Daylong\n",
      "Shtendhnens  -  entrenchments\n",
      "fale  -  faille\n",
      "TRAIN - Epoch: 1  - Step: 7500  - Cost: 16.532211  - CER: 0.500781  - WER (step): 0.875\n",
      "\n",
      "Train examples pred vs real:\n",
      "aASOPEA  -  CASSIOPEIA\n",
      "sAy  -  OSCULATED\n",
      "AUTCOPOn  -  AUTOPILOT\n",
      "Bebing  -  Ebbing\n",
      "Foaiest  -  Foxiest\n",
      "Soboaed  -  Showboated\n",
      "MIOURNERS  -  MOURNERS\n",
      "TRAIN - Epoch: 1  - Step: 8000  - Cost: 15.316027  - CER: 0.46097192  - WER (step): 0.875\n",
      "\n",
      "Train examples pred vs real:\n",
      "bameites  -  bicarbonates\n",
      "Peieaayes  -  Psychoanalytic\n",
      "CosES  -  GRUNEWALD\n",
      "IT  -  RUFFLY\n",
      "Piumpialisn  -  triumphalism\n",
      "dendsetters  -  trendsetters\n",
      "P  -  RETROFIT\n",
      "TRAIN - Epoch: 1  - Step: 8500  - Cost: 14.280104  - CER: 0.4335304  - WER (step): 0.75\n",
      "\n",
      "Train examples pred vs real:\n",
      "inereourse  -  intercourse\n",
      "ENOE  -  enoe\n",
      "lemzed  -  itemized\n",
      "Seudhgere  -  Southgate\n",
      "Camnt  -  Command\n",
      "DAMP  -  DAMP\n",
      "Lyts  -  LACING\n",
      "TRAIN - Epoch: 1  - Step: 9000  - Cost: 13.565559  - CER: 0.41275203  - WER (step): 0.875\n",
      "\n",
      "Train examples pred vs real:\n",
      "iaepes  -  Trooper\n",
      "RENY  -  RENT\n",
      "laer  -  laxer\n",
      "ContesT  -  contest\n",
      "VERENS  -  STERNNESS\n",
      "liopathig  -  Idiopathic\n",
      "RognizancG  -  recognizance\n",
      "Testing epoch: 1\n",
      "TEST - Cost: 12.728888  - CER: 0.3836714  - WER: 0.87109375\n",
      "\n",
      "Test examples pred vs real:\n",
      "PaINESS  -  MEALINESS\n",
      "Tovmie  -  Townie\n",
      "PrerCarefn  -  Overcareful\n",
      "svs  -  Cumulatively\n",
      "PuEy  -  anyway\n",
      "Pliteatiatg  -  existentially\n",
      "PremSed  -  Premised\n",
      "DEROEINg  -  DISROBING\n",
      "Wooer  -  Wooer\n",
      "KissER  -  KISSER\n",
      "Model saved in file: /home/ubuntu/data/oxford_syntetic_text/models/test03/model-1\n",
      "Training epoch: 2\n",
      "TRAIN - Epoch: 2  - Step: 500  - Cost: 12.1165495  - CER: 0.37129605  - WER (step): 0.875\n",
      "\n",
      "Train examples pred vs real:\n",
      "Capeyron  -  Clapeyron\n",
      "Blarmaceauies  -  Pharmaceutics\n",
      "CYUS  -  CYGNUS\n",
      "BPONENT  -  EXPONENT\n",
      "spnniDs  -  SPUMIER\n",
      "Stolidness  -  Stolidness\n",
      "RINCEDOKS  -  PRINCEDOMS\n",
      "TRAIN - Epoch: 2  - Step: 1000  - Cost: 11.620479  - CER: 0.36164  - WER (step): 0.625\n",
      "\n",
      "Train examples pred vs real:\n",
      "Touts  -  Touts\n",
      "Wvdest  -  Vividest\n",
      "Concavity  -  concavity\n",
      "PERrORe  -  PERFORMS\n",
      "Non  -  Noon\n",
      "Transporter  -  Transporter\n",
      "MIRSPAIES  -  WARRANTIES\n",
      "TRAIN - Epoch: 2  - Step: 1500  - Cost: 11.380301  - CER: 0.34783965  - WER (step): 0.8125\n",
      "\n",
      "Train examples pred vs real:\n",
      "Ppate  -  Impala\n",
      "LILIA  -  LILIA\n",
      "Deharked  -  Debarked\n",
      "Polgphane  -  polyphemus\n",
      "gEOMSTANCED  -  CIRCUMSTANCED\n",
      "FEARISAICRL  -  PHARISAICAL\n",
      "Roland  -  Rolland\n",
      "TRAIN - Epoch: 2  - Step: 2000  - Cost: 10.891138  - CER: 0.3372918  - WER (step): 0.8125\n",
      "\n",
      "Train examples pred vs real:\n",
      "MURPLERS  -  MUFFLERS\n",
      "ELbarts  -  Pilchards\n",
      "BiH  -  Loftiest\n",
      "Tety  -  tracery\n",
      "Buoy  -  Buoy\n",
      "BODieaior  -  application\n",
      "POLTBUROS  -  POLITBUROS\n",
      "TRAIN - Epoch: 2  - Step: 2500  - Cost: 10.708041  - CER: 0.33225393  - WER (step): 0.875\n",
      "\n",
      "Train examples pred vs real:\n",
      "MUSKELLUNG  -  MUSKELLUNGE\n",
      "PARACHUIST  -  PARACHUTIST\n",
      "Tenentry  -  Tenantry\n",
      "sIPSRIE  -  IMPOSING\n",
      "Chanelaos  -  Chancellor\n",
      "TAISCRIPTS  -  TRANSCRIPTS\n",
      "MANDALAS  -  MANDALAS\n",
      "TRAIN - Epoch: 2  - Step: 3000  - Cost: 10.472994  - CER: 0.32158172  - WER (step): 0.8125\n",
      "\n",
      "Train examples pred vs real:\n",
      "TRAMIPLED  -  TRAMPLED\n",
      "Outmatrhes  -  outmatches\n",
      "Destepping  -  Overstepping\n",
      "Oerng  -  shuttering\n",
      "Camplaining  -  Complaining\n",
      "GraL  -  CREEL\n",
      "headboard  -  headboard\n",
      "TRAIN - Epoch: 2  - Step: 3500  - Cost: 10.003018  - CER: 0.3093385  - WER (step): 0.8125\n",
      "\n",
      "Train examples pred vs real:\n",
      "Caning  -  Caning\n",
      "cONWAY  -  CONWAY\n",
      "Ors  -  Draperies\n",
      "subsides  -  subsides\n",
      "SEXILY  -  SEXILY\n",
      "WstnsPNy  -  MILITIAMAN\n",
      "Consirucine  -  Constructive\n",
      "TRAIN - Epoch: 2  - Step: 4000  - Cost: 9.780706  - CER: 0.30351558  - WER (step): 0.625\n",
      "\n",
      "Train examples pred vs real:\n",
      "COWRIE  -  COWRIE\n",
      "smowboarding  -  snowboarding\n",
      "beeng  -  benz\n",
      "MACYS  -  Macys\n",
      "beaness  -  leanness\n",
      "Unawares  -  Unawares\n",
      "LAWAI  -  HAWAII\n",
      "TRAIN - Epoch: 2  - Step: 4500  - Cost: 9.452562  - CER: 0.29407758  - WER (step): 0.8125\n",
      "\n",
      "Train examples pred vs real:\n",
      "hrltlng  -  halting\n",
      "cra  -  era\n",
      "Scavenging  -  Scavenging\n",
      "rerooents  -  permanents\n",
      "ircons  -  Zircons\n",
      "Digpesd  -  Dispersal\n",
      "Dimness  -  Dimness\n",
      "TRAIN - Epoch: 2  - Step: 5000  - Cost: 9.364994  - CER: 0.29326832  - WER (step): 0.875\n",
      "\n",
      "Train examples pred vs real:\n",
      "MNCROWAVE  -  MICROWAVE\n",
      "Ointments  -  ointments\n",
      "PEBBLED  -  PEBBLED\n",
      "DaNsEUSES  -  DANSEUSES\n",
      "NEPALESE  -  NEPALESE\n",
      "Dreia  -  hardin\n",
      "Cugering  -  Queering\n",
      "TRAIN - Epoch: 2  - Step: 5500  - Cost: 9.375883  - CER: 0.29304263  - WER (step): 0.6875\n",
      "\n",
      "Train examples pred vs real:\n",
      "fints  -  flints\n",
      "Colitis  -  Colitis\n",
      "RnOOper  -  improper\n",
      "Deppiet  -  peppier\n",
      "AUNCHLNe  -  MUNCHING\n",
      "RAMS  -  RAMS\n",
      "Hllbby  -  flabby\n",
      "TRAIN - Epoch: 2  - Step: 6000  - Cost: 9.026805  - CER: 0.28196585  - WER (step): 0.75\n",
      "\n",
      "Train examples pred vs real:\n",
      "Dysprosium  -  Dysprosium\n",
      "Inpractical  -  impractical\n",
      "GINGED  -  clinched\n",
      "crppe  -  cropper\n",
      "cobstoppers  -  Gobstoppers\n",
      "INRALE  -  Inhale\n",
      "HAddENED  -  maddened\n",
      "TRAIN - Epoch: 2  - Step: 6500  - Cost: 8.795625  - CER: 0.2730014  - WER (step): 0.8125\n",
      "\n",
      "Train examples pred vs real:\n",
      "Bruldlzes  -  Brutalizes\n",
      "MarTOD  -  marrow\n",
      "HTUAL  -  RITUAL\n",
      "Released  -  released\n",
      "Peddss  -  purist\n",
      "mnarking  -  marking\n",
      "ATE  -  APPRAISE\n",
      "TRAIN - Epoch: 2  - Step: 7000  - Cost: 8.681094  - CER: 0.26896235  - WER (step): 0.625\n",
      "\n",
      "Train examples pred vs real:\n",
      "Fnell  -  Knell\n",
      "CONVULSIVELY  -  CONVULSIVELY\n",
      "Cnfit  -  unfit\n",
      "overclock  -  overclock\n",
      "pogo  -  pogo\n",
      "OSHERS  -  JOSHERS\n",
      "Hackguards  -  Blackguards\n",
      "TRAIN - Epoch: 2  - Step: 7500  - Cost: 8.779804  - CER: 0.27426815  - WER (step): 0.875\n",
      "\n",
      "Train examples pred vs real:\n",
      "Spaablocss  -  Spamblocks\n",
      "D  -  20\n",
      "LuTTRG  -  CLIMACTERIC\n",
      "scoFFLANY  -  scofflaw\n",
      "sertuplets  -  Sextuplets\n",
      "Collapsing  -  Collapsing\n",
      "DNSTEaD  -  INSTEAD\n",
      "TRAIN - Epoch: 2  - Step: 8000  - Cost: 8.672175  - CER: 0.26894805  - WER (step): 0.625\n",
      "\n",
      "Train examples pred vs real:\n",
      "Pouches  -  Pouches\n",
      "teaplies  -  yearlies\n",
      "sparsest  -  sparsest\n",
      "Upa  -  upa\n",
      "CRaYONED  -  crayoned\n",
      "lousth  -  kenneth\n",
      "debited  -  debited\n",
      "TRAIN - Epoch: 2  - Step: 8500  - Cost: 8.4327345  - CER: 0.264398  - WER (step): 0.8125\n",
      "\n",
      "Train examples pred vs real:\n",
      "storybaok  -  storybook\n",
      "uttingly  -  cuttingly\n",
      "MONOMANIAS  -  MONOMANIAC\n",
      "BICNEES  -  EXCHANGES\n",
      "OBSeSSIVEN  -  OBSESSIVELY\n",
      "muchincd  -  machined\n",
      "faciitator  -  facilitator\n",
      "TRAIN - Epoch: 2  - Step: 9000  - Cost: 8.330441  - CER: 0.260161  - WER (step): 0.6875\n",
      "\n",
      "Train examples pred vs real:\n",
      "CDTES  -  Cotes\n",
      "GUSTED  -  GUSTED\n",
      "rearmost  -  rearmost\n",
      "BASED  -  BIASED\n",
      "RaCeIeS  -  Racemes\n",
      "STUners  -  Tuners\n",
      "LUNCHROOS  -  LUNCHROOMS\n",
      "Testing epoch: 2\n",
      "TEST - Cost: 8.075815  - CER: 0.25502428  - WER: 0.677734375\n",
      "\n",
      "Test examples pred vs real:\n",
      "HEcASTAR  -  Megastar\n",
      "sharpens  -  sharpens\n",
      "Relse  -  Reuse\n",
      "CHEMICAL  -  CHEMICAL\n",
      "Dekanhy  -  Dashingly\n",
      "Wilae  -  williwaws\n",
      "TRUCES  -  TRUCES\n",
      "gentility  -  gentility\n",
      "Grabby  -  Grabby\n",
      "RaEEEREE  -  TRANSVERSES\n",
      "Model saved in file: /home/ubuntu/data/oxford_syntetic_text/models/test03/model-2\n",
      "Training epoch: 3\n",
      "TRAIN - Epoch: 3  - Step: 500  - Cost: 7.4960165  - CER: 0.23698474  - WER (step): 0.6875\n",
      "\n",
      "Train examples pred vs real:\n",
      "heathman  -  heathman\n",
      "siccd  -  Nicaea\n",
      "collapsible  -  collapsible\n",
      "sa  -  Slaw\n",
      "PENDIG  -  PENDING\n",
      "Benspecdaliztimt  -  nonspecializing\n",
      "Flashguns  -  Flashguns\n",
      "TRAIN - Epoch: 3  - Step: 1000  - Cost: 7.2155633  - CER: 0.23254313  - WER (step): 0.75\n",
      "\n",
      "Train examples pred vs real:\n",
      "jussed  -  Fussed\n",
      "SwaTS  -  Swats\n",
      "TAPInG  -  TAPING\n",
      "OSPREYS  -  OSPREYS\n",
      "vesteryear  -  yesteryear\n",
      "FERNANDD  -  fernando\n",
      "MARSHIALNG  -  MARSHALING\n",
      "TRAIN - Epoch: 3  - Step: 1500  - Cost: 7.3845973  - CER: 0.23487358  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "SWNTHESZING  -  SYNTHESIZING\n",
      "LeapPraise  -  Reappraise\n",
      "Torsos  -  Torsos\n",
      "unsaddled  -  unsaddled\n",
      "mnangoes  -  mangoes\n",
      "VEwpOint  -  Viewpoint\n",
      "sokeE  -  Jouster\n",
      "TRAIN - Epoch: 3  - Step: 2000  - Cost: 7.3589473  - CER: 0.23321918  - WER (step): 0.6875\n",
      "\n",
      "Train examples pred vs real:\n",
      "IEMpERAT  -  intemperate\n",
      "salmonellae  -  salmonellae\n",
      "Pesne  -  Delaying\n",
      "Bstuary  -  Estuary\n",
      "langitnds  -  longitude\n",
      "Peraite  -  Parasitic\n",
      "BANCARNATED  -  REINCARNATED\n",
      "TRAIN - Epoch: 3  - Step: 2500  - Cost: 7.255906  - CER: 0.2307145  - WER (step): 0.875\n",
      "\n",
      "Train examples pred vs real:\n",
      "restablihner  -  reestablishment\n",
      "Omiesions  -  Omissions\n",
      "Adenoitls  -  Adenoids\n",
      "QBTZERS  -  KIBITZERS\n",
      "POTMANTEAU  -  PORTMANTEAU\n",
      "FURLONGS  -  FURLONGS\n",
      "iermaphrodite  -  hermaphroditic\n",
      "TRAIN - Epoch: 3  - Step: 3000  - Cost: 7.240041  - CER: 0.23302837  - WER (step): 0.6875\n",
      "\n",
      "Train examples pred vs real:\n",
      "lonesome  -  lonesome\n",
      "sosier  -  crosier\n",
      "EXTRUDES  -  extrudes\n",
      "Detrier  -  Pettier\n",
      "Touted  -  Touted\n",
      "MPERISHDLE  -  IMPERISHABLE\n",
      "SYSOP  -  SYSOP\n",
      "TRAIN - Epoch: 3  - Step: 3500  - Cost: 7.19937  - CER: 0.22849393  - WER (step): 0.625\n",
      "\n",
      "Train examples pred vs real:\n",
      "palace  -  palace\n",
      "tumoy  -  tumor\n",
      "SEARANG  -  SEMARANG\n",
      "Cnehiroiing  -  stockbroking\n",
      "gibert  -  gilbert\n",
      "abralham  -  abraham\n",
      "PRESERWING  -  PRESERVING\n",
      "TRAIN - Epoch: 3  - Step: 4000  - Cost: 7.197333  - CER: 0.2283383  - WER (step): 0.625\n",
      "\n",
      "Train examples pred vs real:\n",
      "PURCHASABL  -  PURCHASABLE\n",
      "delegated  -  delegated\n",
      "whitman  -  whitman\n",
      "NOTABLY  -  NOTABLY\n",
      "gareered  -  Careered\n",
      "SHRINKABLE  -  SHRINKABLE\n",
      "ldle  -  idle\n",
      "TRAIN - Epoch: 3  - Step: 4500  - Cost: 6.9388456  - CER: 0.22188556  - WER (step): 0.6875\n",
      "\n",
      "Train examples pred vs real:\n",
      "SY  -  SWOOP\n",
      "Qo  -  aol\n",
      "vablw  -  Viably\n",
      "bantus  -  bantus\n",
      "GAndLeS  -  candles\n",
      "Sinker  -  Sinker\n",
      "REGNAnT  -  REGNANT\n",
      "TRAIN - Epoch: 3  - Step: 5000  - Cost: 6.833347  - CER: 0.21876597  - WER (step): 0.75\n",
      "\n",
      "Train examples pred vs real:\n",
      "TeLeTIon  -  TELETHON\n",
      "Hrt  -  Yurt\n",
      "garder  -  guarder\n",
      "centrels  -  centrals\n",
      "fantastic  -  fantastic\n",
      "iestimably  -  inestimably\n",
      "Disimulating  -  Dissimulating\n",
      "TRAIN - Epoch: 3  - Step: 5500  - Cost: 6.90179  - CER: 0.22154674  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "sCLOGD  -  ECLOGUE\n",
      "TIPSIER  -  TIPSIER\n",
      "FORESTS  -  FORESTS\n",
      "howler  -  howler\n",
      "lottie  -  lottie\n",
      "Buyouts  -  Buyouts\n",
      "PRsOTHERAPST  -  PHYSIOTHERAPIST\n",
      "TRAIN - Epoch: 3  - Step: 6000  - Cost: 6.744445  - CER: 0.21499887  - WER (step): 0.625\n",
      "\n",
      "Train examples pred vs real:\n",
      "VEGETATING  -  VEGETATING\n",
      "roted  -  Toked\n",
      "Fippery  -  Frippery\n",
      "ArIstocracy  -  Aristocracy\n",
      "Kuerat  -  Leavens\n",
      "UNDERMINED  -  UNDERMINED\n",
      "Deliverymen  -  Deliverymen\n",
      "TRAIN - Epoch: 3  - Step: 6500  - Cost: 6.9064713  - CER: 0.22072612  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "AINIBIKES  -  MINIBIKES\n",
      "flits  -  flits\n",
      "ConcERMe  -  CONCERNING\n",
      "BACKBTERS  -  BACKBITERS\n",
      "TUNIS  -  TUNIS\n",
      "GHtMOTHERAEY  -  CHEMOTHERAPY\n",
      "CVET  -  CIVET\n",
      "TRAIN - Epoch: 3  - Step: 7000  - Cost: 6.7305217  - CER: 0.21475127  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "RANBOARDS  -  DRAINBOARDS\n",
      "diskiust  -  duskiest\n",
      "laundry  -  Laundry\n",
      "TUrRBOCHARGERS  -  TURBOCHARGERS\n",
      "bremen  -  bremen\n",
      "Vaporize  -  Vaporize\n",
      "WAGER  -  WAGER\n",
      "TRAIN - Epoch: 3  - Step: 7500  - Cost: 6.9851017  - CER: 0.22680564  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "Rvroted  -  Penetrated\n",
      "Pressie  -  Pressie\n",
      "inventr  -  inventor\n",
      "RECIPROCATING  -  RECIPROCATING\n",
      "bazbeaur  -  bazbeaux\n",
      "doltish  -  doltish\n",
      "SHOUTER  -  SHOUTER\n",
      "TRAIN - Epoch: 3  - Step: 8000  - Cost: 6.593973  - CER: 0.21130514  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "OurACITORIES  -  OLFACTORIES\n",
      "sabbaths  -  sabbaths\n",
      "OMaR  -  OMAR\n",
      "droplets  -  droplets\n",
      "Gome  -  Geomagnetic\n",
      "evildoer  -  evildoer\n",
      "TeetRTS  -  Jesters\n",
      "TRAIN - Epoch: 3  - Step: 8500  - Cost: 6.757958  - CER: 0.21404848  - WER (step): 0.6875\n",
      "\n",
      "Train examples pred vs real:\n",
      "AUTERT  -  austerity\n",
      "SRHEPTOCOSCAL  -  STREPTOCOCCAL\n",
      "DISLODGES  -  DISLODGES\n",
      "VEW  -  VIEW\n",
      "CHANCELLORSvILIE  -  Chancellorsville\n",
      "CHORORNOROCAREN  -  CHLOROFLUOROCARBON\n",
      "EyVEyE  -  anyways\n",
      "TRAIN - Epoch: 3  - Step: 9000  - Cost: 6.667889  - CER: 0.21201065  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "deathezneel  -  Leatherneck\n",
      "Sbuly  -  study\n",
      "THUMBPRINT  -  THUMBPRINT\n",
      "Hckinne  -  Mckinney\n",
      "CRUCIAL  -  CRUCIAL\n",
      "pension  -  pension\n",
      "Seminoles  -  Seminoles\n",
      "Testing epoch: 3\n",
      "TEST - Cost: 7.045307  - CER: 0.232079  - WER: 0.62451171875\n",
      "\n",
      "Test examples pred vs real:\n",
      "Aduthood  -  Adulthood\n",
      "chiropody  -  Chiropody\n",
      "Meteorologic  -  Meteorologic\n",
      "Creeplness  -  Creepiness\n",
      "UosUED  -  DEPRESSURIZED\n",
      "Sgrets  -  Signets\n",
      "camulates  -  accumulation\n",
      "marina  -  marina\n",
      "BURAL  -  burial\n",
      "IRAISOuG  -  INFRASONIC\n",
      "Model saved in file: /home/ubuntu/data/oxford_syntetic_text/models/test03/model-3\n",
      "Training epoch: 4\n",
      "TRAIN - Epoch: 4  - Step: 500  - Cost: 5.6714907  - CER: 0.18524598  - WER (step): 0.25\n",
      "\n",
      "Train examples pred vs real:\n",
      "adjuration  -  adjuration\n",
      "EoOS  -  EXHUMATION\n",
      "JOURNAL  -  Journal\n",
      "Tarps  -  Tarps\n",
      "Backstabber  -  Backstabber\n",
      "PROSELYTED  -  PROSELYTED\n",
      "ramovO  -  ramova\n",
      "TRAIN - Epoch: 4  - Step: 1000  - Cost: 5.8014464  - CER: 0.19153015  - WER (step): 0.25\n",
      "\n",
      "Train examples pred vs real:\n",
      "cascaded  -  cascaded\n",
      "rattrap  -  rattrap\n",
      "walkway  -  walkway\n",
      "Humid  -  Humid\n",
      "earldom  -  earldom\n",
      "grpped  -  gripped\n",
      "txstl  -  Unproductively\n",
      "TRAIN - Epoch: 4  - Step: 1500  - Cost: 5.7928576  - CER: 0.19150628  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "VAnnED  -  VANNED\n",
      "Biannual  -  Biannual\n",
      "Jacobson  -  Jacobson\n",
      "micheal  -  micheal\n",
      "DISC  -  DISC\n",
      "ADY  -  ady\n",
      "Ehermet  -  Ethernet\n",
      "TRAIN - Epoch: 4  - Step: 2000  - Cost: 5.9026937  - CER: 0.19437452  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "Satirize  -  Satirize\n",
      "DENEFtS  -  BENEFITS\n",
      "SPAnIEs  -  SPARKIEST\n",
      "Atradtively  -  Attractively\n",
      "HUSKEd  -  HUSKED\n",
      "Flavorings  -  Flavorings\n",
      "hntermingle  -  intermingle\n",
      "TRAIN - Epoch: 4  - Step: 2500  - Cost: 5.7787147  - CER: 0.19149728  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "Seaborg  -  seaborg\n",
      "cotter  -  cotter\n",
      "Prmeial  -  Parricidal\n",
      "mpole  -  maypole\n",
      "Recces  -  Recces\n",
      "DEFILEMENT  -  DEFILEMENT\n",
      "BRENTON  -  BRENTON\n",
      "TRAIN - Epoch: 4  - Step: 3000  - Cost: 5.75595  - CER: 0.18894185  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "disburses  -  disburses\n",
      "Refrigerator  -  Refrigerator\n",
      "UnOTsHiEY  -  KUIBYSHEV\n",
      "POPULARIZES  -  POPULARIZES\n",
      "Eartnsheking  -  Earthshaking\n",
      "questioned  -  questioned\n",
      "COMMUNICANTS  -  COMMUNICANTS\n",
      "TRAIN - Epoch: 4  - Step: 3500  - Cost: 5.902918  - CER: 0.19217606  - WER (step): 0.625\n",
      "\n",
      "Train examples pred vs real:\n",
      "lamings  -  flamings\n",
      "tehydrates  -  dehydrates\n",
      "Sectary  -  Sectary\n",
      "CANAPE  -  CANAPE\n",
      "Ramp  -  Ramp\n",
      "preides  -  presides\n",
      "BEROGATION  -  ABROGATION\n",
      "TRAIN - Epoch: 4  - Step: 4000  - Cost: 5.846954  - CER: 0.19183232  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "Springiness  -  Springiness\n",
      "promotions  -  promotions\n",
      "Fluorescing  -  fluorescing\n",
      "Orated  -  Orated\n",
      "trampoline  -  trampoline\n",
      "REAmS  -  Pravus\n",
      "TrVIDIOUsLY  -  INVIDIOUSLY\n",
      "TRAIN - Epoch: 4  - Step: 4500  - Cost: 5.91248  - CER: 0.19136031  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "OUTPOINT  -  OUTPOINT\n",
      "Wsherete  -  usherette\n",
      "Leaky  -  Leaky\n",
      "THING  -  THING\n",
      "Signalizes  -  Signalizes\n",
      "Rete  -  Radar\n",
      "PETITIONED  -  PETITIONED\n",
      "TRAIN - Epoch: 4  - Step: 5000  - Cost: 5.829629  - CER: 0.19306815  - WER (step): 0.375\n",
      "\n",
      "Train examples pred vs real:\n",
      "Lawfulness  -  Lawfulness\n",
      "SYMBOLIG  -  SYMBOLIC\n",
      "Fronds  -  Fronds\n",
      "Orerteacting  -  Overreacting\n",
      "peddler  -  peddler\n",
      "REINS  -  REINS\n",
      "rebroadcast  -  rebroadcast\n",
      "TRAIN - Epoch: 4  - Step: 5500  - Cost: 5.811638  - CER: 0.18674944  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "Wnreale  -  Unseals\n",
      "TAKEOFF  -  TAKEOFF\n",
      "RerUdiATe  -  Repudiate\n",
      "FAIRLY  -  FAIRLY\n",
      "consists  -  consists\n",
      "appending  -  appending\n",
      "Wiscatorial  -  piscatorial\n",
      "TRAIN - Epoch: 4  - Step: 6000  - Cost: 5.751504  - CER: 0.1867997  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "coNCEVE  -  CONCEIVE\n",
      "Mu  -  Mu\n",
      "OL  -  OPAL\n",
      "Lotn  -  Loan\n",
      "beck  -  beck\n",
      "etiologies  -  etiologies\n",
      "Minuting  -  Minuting\n",
      "TRAIN - Epoch: 4  - Step: 6500  - Cost: 5.924909  - CER: 0.19201489  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "Ms  -  Loaf\n",
      "CHASED  -  CHASED\n",
      "CHIEFTAINS  -  CHIEFTAINS\n",
      "FENDER  -  FENDER\n",
      "BENGALS  -  BENGALS\n",
      "UnCnimty  -  unanimity\n",
      "Pridium  -  iridium\n",
      "TRAIN - Epoch: 4  - Step: 7000  - Cost: 5.711339  - CER: 0.18982247  - WER (step): 0.75\n",
      "\n",
      "Train examples pred vs real:\n",
      "ASsaying  -  Assaying\n",
      "DNPiPES  -  DRAINPIPES\n",
      "Trapdoer  -  Trapdoor\n",
      "suPERmEN  -  SUPERMEN\n",
      "GONGEALMENT  -  CONCEALMENT\n",
      "aOMBARDMENT  -  BOMBARDMENT\n",
      "Whelped  -  Whelped\n",
      "TRAIN - Epoch: 4  - Step: 7500  - Cost: 5.6133385  - CER: 0.18096691  - WER (step): 0.625\n",
      "\n",
      "Train examples pred vs real:\n",
      "CEBUANO  -  CEBUANO\n",
      "Quilter  -  Quilter\n",
      "Stagnat  -  Stagnate\n",
      "ineptly  -  ineptly\n",
      "HOmuNior  -  NONUNION\n",
      "idbomatically  -  idiomatically\n",
      "Recentest  -  Recentest\n",
      "TRAIN - Epoch: 4  - Step: 8000  - Cost: 5.778626  - CER: 0.18806449  - WER (step): 0.3125\n",
      "\n",
      "Train examples pred vs real:\n",
      "Impaired  -  Impaired\n",
      "Apropriations  -  Appropriations\n",
      "SHORTSTOPS  -  SHORTSTOPS\n",
      "CURLICUED  -  CURLICUED\n",
      "poignant  -  poignant\n",
      "Cornfields  -  Cornfields\n",
      "Refurbishes  -  Refurbishes\n",
      "TRAIN - Epoch: 4  - Step: 8500  - Cost: 5.6961775  - CER: 0.1876857  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "RepuLsIvENESS  -  REPULSIVENESS\n",
      "Functionalists  -  Functionalists\n",
      "NONVOCATIONA  -  NONVOCATIONAL\n",
      "diocesan  -  diocesan\n",
      "AnLoHy  -  sellotaping\n",
      "FEEHS  -  PETERS\n",
      "Snock  -  Smock\n",
      "TRAIN - Epoch: 4  - Step: 9000  - Cost: 5.660532  - CER: 0.18476166  - WER (step): 0.8125\n",
      "\n",
      "Train examples pred vs real:\n",
      "SHUDDENED  -  SHUDDERED\n",
      "Tlde  -  Tilde\n",
      "dealizes  -  Idealizes\n",
      "AEtoicRg  -  METHAMPHETAMINE\n",
      "Eutectic  -  Eutectic\n",
      "Feheries  -  fisheries\n",
      "SYMBoLiGALLY  -  SYMBOLICALLY\n",
      "Testing epoch: 4\n",
      "TEST - Cost: 6.102847  - CER: 0.19424246  - WER: 0.54931640625\n",
      "\n",
      "Test examples pred vs real:\n",
      "GLIMPSE  -  GLIMPSE\n",
      "Stability  -  stability\n",
      "SousncihvoAa  -  Councilwoman\n",
      "LIKELIER  -  LIKELIER\n",
      "legrooms  -  legrooms\n",
      "Soming  -  Booming\n",
      "Leapfrogs  -  Leapfrogs\n",
      "brahmanism  -  brahmanism\n",
      "lses  -  lasses\n",
      "Peony  -  Peony\n",
      "Model saved in file: /home/ubuntu/data/oxford_syntetic_text/models/test03/model-4\n",
      "Training epoch: 5\n",
      "TRAIN - Epoch: 5  - Step: 500  - Cost: 4.8327284  - CER: 0.16181011  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "Fitchis  -  Litchis\n",
      "Admission  -  Admission\n",
      "CETHOGRAeHCALY  -  ORTHOGRAPHICALLY\n",
      "BOWLINES  -  BOWLINES\n",
      "PATHETICALY  -  PATHETICALLY\n",
      "WINIER  -  Winier\n",
      "PRISMS  -  PRISMS\n",
      "TRAIN - Epoch: 5  - Step: 1000  - Cost: 5.0880237  - CER: 0.16811088  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "BLeNCHRED  -  BLENCHED\n",
      "Cavert  -  Calvert\n",
      "libeled  -  libeled\n",
      "Ileared  -  Cleared\n",
      "wavier  -  wavier\n",
      "Slipknot  -  Slipknot\n",
      "ALLUSIVE  -  ALLUSIVE\n",
      "TRAIN - Epoch: 5  - Step: 1500  - Cost: 4.8715177  - CER: 0.16291134  - WER (step): 0.3125\n",
      "\n",
      "Train examples pred vs real:\n",
      "MAUDLIN  -  MAUDLIN\n",
      "MAZAMA  -  MAZAMA\n",
      "Dedauehees  -  Debauchees\n",
      "sunbathed  -  sunbathed\n",
      "Haggard  -  Haggard\n",
      "SPECTROSCOPE  -  SPECTROSCOPE\n",
      "indesecribable  -  indescribable\n",
      "TRAIN - Epoch: 5  - Step: 2000  - Cost: 4.881193  - CER: 0.16194434  - WER (step): 0.375\n",
      "\n",
      "Train examples pred vs real:\n",
      "NIGLkTDRESSES  -  NIGHTDRESSES\n",
      "Receipting  -  Receipting\n",
      "Msting  -  infesting\n",
      "JACKKNIFING  -  JACKKNIFING\n",
      "tarbel  -  tarbell\n",
      "WEAVING  -  WEAVING\n",
      "decipherable  -  decipherable\n",
      "TRAIN - Epoch: 5  - Step: 2500  - Cost: 4.8269153  - CER: 0.16132635  - WER (step): 0.1875\n",
      "\n",
      "Train examples pred vs real:\n",
      "LOCANDA  -  LOCANDA\n",
      "DARTOZnN  -  CARTOON\n",
      "LOOPY  -  LOOPY\n",
      "nonskid  -  nonskid\n",
      "BRAINLESS  -  BRAINLESS\n",
      "Epoxies  -  Epoxies\n",
      "Legends  -  Legends\n",
      "TRAIN - Epoch: 5  - Step: 3000  - Cost: 4.810408  - CER: 0.16152532  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "pesoS  -  pesos\n",
      "faininess  -  faintness\n",
      "MAG  -  MAG\n",
      "EDEN  -  EDEN\n",
      "CORNWALlS  -  CORNWALLIS\n",
      "CONSPECTUS  -  CONSPECTUS\n",
      "CRoiotiags  -  Circulations\n",
      "TRAIN - Epoch: 5  - Step: 3500  - Cost: 4.97621  - CER: 0.16819888  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "Maiestey  -  Mainstay\n",
      "IMNIGRNTS  -  IMMIGRANTS\n",
      "eoneaARN  -  forewarn\n",
      "voGuES  -  VOGUES\n",
      "HERMITS  -  HERMITS\n",
      "Wholesalors  -  Wholesalers\n",
      "COPPERS  -  COPPERS\n",
      "TRAIN - Epoch: 5  - Step: 4000  - Cost: 4.962827  - CER: 0.16706643  - WER (step): 0.3125\n",
      "\n",
      "Train examples pred vs real:\n",
      "zonked  -  zonked\n",
      "DEARIES  -  DEARIES\n",
      "ballot  -  ballot\n",
      "nymphomanl  -  nymphomania\n",
      "Neater  -  Neater\n",
      "CEns  -  beacons\n",
      "CROOKING  -  CROOKING\n",
      "TRAIN - Epoch: 5  - Step: 4500  - Cost: 4.9980636  - CER: 0.16437079  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "Cremates  -  Cremates\n",
      "Mermgraed  -  Monogrammed\n",
      "rcns  -  dbms\n",
      "wellie  -  wellie\n",
      "maughts  -  naughts\n",
      "strode  -  Strode\n",
      "revarns  -  rewarms\n",
      "TRAIN - Epoch: 5  - Step: 5000  - Cost: 4.8821945  - CER: 0.16226383  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "TROLIPE  -  TROUPE\n",
      "doe  -  Jove\n",
      "REOpROACHiING  -  REPROACHING\n",
      "Submersing  -  Submerging\n",
      "Sternums  -  Sternums\n",
      "Swellest  -  Swellest\n",
      "Embryologist  -  Embryologist\n",
      "TRAIN - Epoch: 5  - Step: 5500  - Cost: 5.021497  - CER: 0.16543895  - WER (step): 0.3125\n",
      "\n",
      "Train examples pred vs real:\n",
      "HILLARY  -  HILLARY\n",
      "transoms  -  transoms\n",
      "Pests  -  Pests\n",
      "PIGGED  -  PIGGED\n",
      "Touchy  -  Touchy\n",
      "PERSPIRES  -  PERSPIRES\n",
      "VERIFIED  -  VERIFIED\n",
      "TRAIN - Epoch: 5  - Step: 6000  - Cost: 4.9432163  - CER: 0.16607891  - WER (step): 0.75\n",
      "\n",
      "Train examples pred vs real:\n",
      "NANTES  -  INANITIES\n",
      "ALtERATIONS  -  ALTERATIONS\n",
      "LIbrarIANS  -  Librarians\n",
      "IATETOS  -  INFATUATIONS\n",
      "tecunlees  -  accumulates\n",
      "tailight  -  taillight\n",
      "Inanimately  -  Inanimately\n",
      "TRAIN - Epoch: 5  - Step: 6500  - Cost: 5.0927176  - CER: 0.17179565  - WER (step): 0.375\n",
      "\n",
      "Train examples pred vs real:\n",
      "belonging  -  belonging\n",
      "salotken  -  Solotken\n",
      "BOMBeR  -  BOMBER\n",
      "Heredlitary  -  Hereditary\n",
      "Bisexual  -  Bisexual\n",
      "burnish  -  burnish\n",
      "Cyped  -  Gyved\n",
      "TRAIN - Epoch: 5  - Step: 7000  - Cost: 4.9632893  - CER: 0.16687545  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "Apportlonad  -  Apportioned\n",
      "cheating  -  cheating\n",
      "RETOOLING  -  RETOOLING\n",
      "Ethopian  -  Ethopian\n",
      "transsexual  -  transsexual\n",
      "FIELDING  -  FIELDING\n",
      "Retro  -  Retro\n",
      "TRAIN - Epoch: 5  - Step: 7500  - Cost: 4.9782763  - CER: 0.16505432  - WER (step): 0.75\n",
      "\n",
      "Train examples pred vs real:\n",
      "uhisperers  -  whisperers\n",
      "ECNe  -  ACNE\n",
      "Lyer  -  Iyer\n",
      "Indite  -  Indite\n",
      "tesfred  -  falsified\n",
      "PANTD  -  PANTO\n",
      "Plodder  -  Plodder\n",
      "TRAIN - Epoch: 5  - Step: 8000  - Cost: 4.8824935  - CER: 0.16259135  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "harbou  -  harbour\n",
      "ONSTRUCTOR  -  CONSTRUCTOR\n",
      "Hilometers  -  Kilometers\n",
      "Hygiene  -  Hygiene\n",
      "Compukories  -  Compulsories\n",
      "MILLENNIUM  -  MILLENNIUM\n",
      "MUMMY  -  MUMMY\n",
      "TRAIN - Epoch: 5  - Step: 8500  - Cost: 5.046124  - CER: 0.16789177  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "ACHEsON  -  ACHESON\n",
      "Bathed  -  Bathed\n",
      "Fours  -  Fours\n",
      "Hnrels  -  advancements\n",
      "CORKUPTEH  -  CORRUPTER\n",
      "CRASSER  -  CRASSER\n",
      "IMPELLED  -  IMPELLED\n",
      "TRAIN - Epoch: 5  - Step: 9000  - Cost: 4.9402156  - CER: 0.16999395  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "UNINHITRITED  -  UNINHIBITED\n",
      "madcap  -  madcap\n",
      "absCuraTthie  -  obscurities\n",
      "Opposed  -  Opposed\n",
      "ainifloppiese  -  Minifloppieses\n",
      "taconteurs  -  raconteurs\n",
      "Judicious  -  Judicious\n",
      "Testing epoch: 5\n",
      "TEST - Cost: 5.8354535  - CER: 0.18938038  - WER: 0.53271484375\n",
      "\n",
      "Test examples pred vs real:\n",
      "skined  -  skived\n",
      "CAPTURE  -  CAPTURE\n",
      "ineducabte  -  ineducable\n",
      "LAPS  -  LAPS\n",
      "FISHWIFE  -  FISHWIFE\n",
      "Slating  -  Slating\n",
      "PRENISA  -  PRENSA\n",
      "SHIPWRICHT  -  SHIPWRIGHT\n",
      "paobi  -  jacobi\n",
      "winegrower  -  winegrower\n",
      "Model saved in file: /home/ubuntu/data/oxford_syntetic_text/models/test03/model-5\n",
      "Training epoch: 6\n",
      "TRAIN - Epoch: 6  - Step: 500  - Cost: 4.1377983  - CER: 0.14263263  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "congers  -  congers\n",
      "MISsOURIan  -  MISSOURIAN\n",
      "Dienmbodiment  -  Disembodiment\n",
      "Bemp  -  Hump\n",
      "luigi  -  luigi\n",
      "Tutting  -  Tutting\n",
      "imstetag  -  imstetag\n",
      "TRAIN - Epoch: 6  - Step: 1000  - Cost: 4.133939  - CER: 0.14066285  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "steadiness  -  steadiness\n",
      "RELAELS  -  RELABELS\n",
      "overstated  -  overstated\n",
      "CORD  -  CORD\n",
      "Wrongdoers  -  wrongdoers\n",
      "Nicer  -  Nicer\n",
      "ripped  -  ripped\n",
      "TRAIN - Epoch: 6  - Step: 1500  - Cost: 4.2437263  - CER: 0.14338686  - WER (step): 0.3125\n",
      "\n",
      "Train examples pred vs real:\n",
      "removal  -  removal\n",
      "dionibosns  -  firstborns\n",
      "Disported  -  Disported\n",
      "REFFED  -  REFFED\n",
      "maratha  -  maratha\n",
      "Teasel  -  Teasel\n",
      "RESISTLESS  -  RESISTLESS\n",
      "TRAIN - Epoch: 6  - Step: 2000  - Cost: 4.1944785  - CER: 0.14185326  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "Srdurable  -  Perdurable\n",
      "MECESSITY  -  NECESSITY\n",
      "RUSTICITY  -  RUSTICITY\n",
      "impressive  -  impressive\n",
      "Fizl  -  Frizz\n",
      "whiten  -  Whiten\n",
      "FURSD  -  PURSUED\n",
      "TRAIN - Epoch: 6  - Step: 2500  - Cost: 4.214797  - CER: 0.14287294  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "FORNICATED  -  FORNICATED\n",
      "Outlay  -  Outlay\n",
      "Fasts  -  Fasts\n",
      "Satelted  -  Satellited\n",
      "impiutes  -  imputes\n",
      "DAMONDBACK  -  DIAMONDBACK\n",
      "Empornd  -  tempering\n",
      "TRAIN - Epoch: 6  - Step: 3000  - Cost: 4.2070217  - CER: 0.14311641  - WER (step): 0.375\n",
      "\n",
      "Train examples pred vs real:\n",
      "Dinkies  -  Dinkies\n",
      "acanthuses  -  acanthuses\n",
      "teueyS  -  Leaders\n",
      "unmemorable  -  unmemorable\n",
      "Nee  -  Nee\n",
      "Asses  -  Asses\n",
      "notate  -  notate\n",
      "TRAIN - Epoch: 6  - Step: 3500  - Cost: 4.2755656  - CER: 0.14236319  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "CrucT  -  CHURCH\n",
      "Asolution  -  Absolution\n",
      "rakeawvays  -  takeaways\n",
      "Schrpate  -  Serrate\n",
      "MANSLAUGHTER  -  MANSLAUGHTER\n",
      "UNSETTLES  -  UNSETTLES\n",
      "Gamester  -  Gamester\n",
      "TRAIN - Epoch: 6  - Step: 4000  - Cost: 4.3741584  - CER: 0.1496136  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "INGRAN  -  INGRAIN\n",
      "codex  -  codex\n",
      "significations  -  significations\n",
      "Titenium  -  Titanium\n",
      "ABDUL  -  ABDUL\n",
      "WINDIER  -  WINDIER\n",
      "Whimnsicadlly  -  Whimsically\n",
      "TRAIN - Epoch: 6  - Step: 4500  - Cost: 4.3591356  - CER: 0.14883949  - WER (step): 0.3125\n",
      "\n",
      "Train examples pred vs real:\n",
      "UNEASIER  -  UNEASIER\n",
      "Waldos  -  Waldos\n",
      "Painted  -  Painted\n",
      "Determination  -  Determination\n",
      "koctuntiog  -  Hoodwinking\n",
      "Runs  -  Runs\n",
      "mosSIpIG  -  Gossiping\n",
      "TRAIN - Epoch: 6  - Step: 5000  - Cost: 4.38048  - CER: 0.1496348  - WER (step): 0.6875\n",
      "\n",
      "Train examples pred vs real:\n",
      "TIRAQI  -  IRAQI\n",
      "NUNE  -  NUNEZ\n",
      "CORISTIANS  -  CHRISTIANS\n",
      "Daring  -  Daring\n",
      "Jamest  -  Tamest\n",
      "hoardled  -  hoarded\n",
      "imPrudence  -  imprudence\n",
      "TRAIN - Epoch: 6  - Step: 5500  - Cost: 4.4238253  - CER: 0.1482385  - WER (step): 0.3125\n",
      "\n",
      "Train examples pred vs real:\n",
      "matches  -  matches\n",
      "Dentar  -  Dental\n",
      "bollard  -  bollard\n",
      "VICTIMS  -  VICTIMS\n",
      "decorated  -  decorated\n",
      "CODPIECE  -  CODPIECE\n",
      "wINCING  -  WINCING\n",
      "TRAIN - Epoch: 6  - Step: 6000  - Cost: 4.3814135  - CER: 0.150983  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "fatlock  -  fetlock\n",
      "Gladdest  -  Gladdest\n",
      "PaLmSTrY  -  PALMISTRY\n",
      "utewarmly  -  lukewarmly\n",
      "REPRESINT  -  REPRESENT\n",
      "foolproct  -  foolproof\n",
      "Cala  -  Cala\n",
      "TRAIN - Epoch: 6  - Step: 6500  - Cost: 4.3588986  - CER: 0.14936475  - WER (step): 0.25\n",
      "\n",
      "Train examples pred vs real:\n",
      "Hurding  -  Hurdling\n",
      "unaltered  -  unaltered\n",
      "Protection  -  Protection\n",
      "algae  -  algae\n",
      "etlAisy  -  outlandishly\n",
      "PENSIONS  -  PENSIONS\n",
      "Advantaged  -  Advantaged\n",
      "TRAIN - Epoch: 6  - Step: 7000  - Cost: 4.372674  - CER: 0.14858654  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "Squcakines  -  squeakiness\n",
      "Pomposity  -  Pomposity\n",
      "Eilbustering  -  Filibustering\n",
      "chiropractor  -  chiropractor\n",
      "lover  -  louver\n",
      "Manhar  -  Manhar\n",
      "GLMEBT  -  GLUMMEST\n",
      "TRAIN - Epoch: 6  - Step: 7500  - Cost: 4.3564982  - CER: 0.14924225  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "BLINDER  -  BLINDER\n",
      "TABOOED  -  TABOOED\n",
      "parallelogram  -  parallelogram\n",
      "marmoread  -  Marmoreal\n",
      "RANCHInG  -  Ranching\n",
      "Ride  -  Ride\n",
      "counselling  -  counselling\n",
      "TRAIN - Epoch: 6  - Step: 8500  - Cost: 4.4574914  - CER: 0.15050185  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "oilcioth  -  Oilcloth\n",
      "seped  -  Steeped\n",
      "RIZAL  -  RIZAL\n",
      "Ratbos  -  Bathos\n",
      "DEGENERATED  -  DEGENERATED\n",
      "Players  -  Players\n",
      "posSeR  -  dosser\n",
      "TRAIN - Epoch: 6  - Step: 9000  - Cost: 4.465163  - CER: 0.15072708  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "Mmonophonic  -  monophonic\n",
      "AMiITY  -  AMITY\n",
      "biker  -  biker\n",
      "Disoppeurunce  -  Disappearance\n",
      "Robuster  -  Robuster\n",
      "FRETS  -  FRETS\n",
      "RAInING  -  RAINING\n",
      "Testing epoch: 6\n",
      "TEST - Cost: 6.0577497  - CER: 0.2005625  - WER: 0.54833984375\n",
      "\n",
      "Test examples pred vs real:\n",
      "Alete  -  Mute\n",
      "Overshot  -  Overshot\n",
      "biocheuists  -  biochemists\n",
      "Overwarked  -  Overworked\n",
      "Fiftieth  -  Fiftieth\n",
      "halogens  -  halogens\n",
      "Rdet  -  Labials\n",
      "capitalize  -  capitalize\n",
      "Atonal  -  Atonal\n",
      "AWAKENING  -  AWAKENING\n",
      "Model saved in file: /home/ubuntu/data/oxford_syntetic_text/models/test03/model-6\n",
      "Training epoch: 7\n",
      "TRAIN - Epoch: 7  - Step: 500  - Cost: 3.4662578  - CER: 0.12058305  - WER (step): 0.3125\n",
      "\n",
      "Train examples pred vs real:\n",
      "SyllABLE  -  Syllable\n",
      "BLudERbUS  -  Blunderbuss\n",
      "SHADES  -  SHADES\n",
      "guatemalans  -  guatemalans\n",
      "ISTRUMENIS  -  INSTRUMENTS\n",
      "faff  -  faff\n",
      "Eections  -  Ejections\n",
      "TRAIN - Epoch: 7  - Step: 1000  - Cost: 3.4775496  - CER: 0.121187255  - WER (step): 0.3125\n",
      "\n",
      "Train examples pred vs real:\n",
      "UNSNAP  -  UNSNAP\n",
      "CB  -  CB\n",
      "locations  -  locations\n",
      "Spire  -  Spire\n",
      "antertaining  -  entertaining\n",
      "meRNEReeS  -  DOCUMENTARIES\n",
      "recting  -  racking\n",
      "TRAIN - Epoch: 7  - Step: 1500  - Cost: 3.6600277  - CER: 0.12443053  - WER (step): 0.1875\n",
      "\n",
      "Train examples pred vs real:\n",
      "SPELEOLOGICAL  -  SPELEOLOGICAL\n",
      "charles  -  charles\n",
      "paradise  -  paradise\n",
      "Medcafey  -  Mcdaniel\n",
      "Dispiriting  -  Dispiriting\n",
      "deputations  -  deputations\n",
      "DOT  -  DOT\n",
      "TRAIN - Epoch: 7  - Step: 2000  - Cost: 3.6945307  - CER: 0.12818924  - WER (step): 0.375\n",
      "\n",
      "Train examples pred vs real:\n",
      "INLGbitTy  -  INELIGIBILITY\n",
      "DEFUSED  -  DIFFUSED\n",
      "Johnnie  -  Johnnie\n",
      "Overpay  -  Overpay\n",
      "distinctively  -  distinctively\n",
      "IEALDEE  -  WILDFIRE\n",
      "klee  -  klee\n",
      "TRAIN - Epoch: 7  - Step: 2500  - Cost: 3.675247  - CER: 0.12721229  - WER (step): 0.5625\n",
      "\n",
      "Train examples pred vs real:\n",
      "SEPARATE  -  SEPARATE\n",
      "Sconpions  -  Scorpions\n",
      "TransceNdenE  -  Transcendence\n",
      "Speleological  -  Speleological\n",
      "fOONELINESS  -  LOVELINESS\n",
      "sundanely  -  Mundanely\n",
      "RenEWVABLE  -  Renewable\n",
      "TRAIN - Epoch: 7  - Step: 3000  - Cost: 3.7182398  - CER: 0.13064092  - WER (step): 0.375\n",
      "\n",
      "Train examples pred vs real:\n",
      "DISABUSE  -  DISABUSE\n",
      "michelle  -  michelle\n",
      "gOAnO  -  groans\n",
      "thinnest  -  thinnest\n",
      "bureaucracies  -  bureaucracies\n",
      "FLIER  -  FLIER\n",
      "Jamie  -  Jamie\n",
      "TRAIN - Epoch: 7  - Step: 3500  - Cost: 3.8413043  - CER: 0.13243225  - WER (step): 0.1875\n",
      "\n",
      "Train examples pred vs real:\n",
      "appeases  -  appeases\n",
      "bernd  -  bernd\n",
      "DEPOLARIZATION  -  DEPOLARIZATION\n",
      "Fleece  -  Fleece\n",
      "Skids  -  Skids\n",
      "Fistful  -  Fistful\n",
      "suITcase  -  Suitcase\n",
      "TRAIN - Epoch: 7  - Step: 4000  - Cost: 3.8783276  - CER: 0.13643527  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "Herakles  -  Herakles\n",
      "Sates  -  Ingrates\n",
      "mscalling  -  miscalling\n",
      "Purerr  -  Purer\n",
      "BERNSTEIN  -  BERNSTEIN\n",
      "Ianiost  -  loamiest\n",
      "Dedications  -  Dedications\n",
      "TRAIN - Epoch: 7  - Step: 4500  - Cost: 3.8436744  - CER: 0.13265616  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "proms  -  proms\n",
      "AssocIaIiVe  -  Associative\n",
      "RECONSIGNING  -  RECONSIGNING\n",
      "AMBIC  -  IAMBIC\n",
      "DIANONDS  -  DIAMONDS\n",
      "partieipate  -  participate\n",
      "arrangers  -  arrangers\n",
      "TRAIN - Epoch: 7  - Step: 5000  - Cost: 3.7686284  - CER: 0.13048752  - WER (step): 0.625\n",
      "\n",
      "Train examples pred vs real:\n",
      "Rents  -  Rents\n",
      "TUCkIng  -  tucking\n",
      "MMarcia  -  Marcia\n",
      "HARNESSES  -  HARNESSES\n",
      "CAdENCED  -  Cadenced\n",
      "Strike  -  strike\n",
      "DRAFTSMOMEG  -  DRAFTSWOMEN\n",
      "TRAIN - Epoch: 7  - Step: 5500  - Cost: 3.886708  - CER: 0.13331464  - WER (step): 0.25\n",
      "\n",
      "Train examples pred vs real:\n",
      "sterilizer  -  sterilizer\n",
      "Cufifences  -  Confidences\n",
      "MUSES  -  MUSES\n",
      "apostasies  -  apostasies\n",
      "IaSACHUSETTS  -  MASSACHUSETTS\n",
      "Unfitted  -  Unfitted\n",
      "Miasmc  -  Miasma\n",
      "TRAIN - Epoch: 7  - Step: 6000  - Cost: 3.8941245  - CER: 0.13217601  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "Mencius  -  Mencius\n",
      "mARCOIC  -  NARCOTIC\n",
      "Figs  -  Figs\n",
      "indirectly  -  indirectly\n",
      "Groupers  -  Groupers\n",
      "CREDENCS  -  CREDENCE\n",
      "Suspension  -  Suspension\n",
      "TRAIN - Epoch: 7  - Step: 6500  - Cost: 3.821488  - CER: 0.1331894  - WER (step): 0.625\n",
      "\n",
      "Train examples pred vs real:\n",
      "WIRED  -  WIRED\n",
      "BlaNDO  -  Blando\n",
      "CumBeriNG  -  cumbering\n",
      "Loyally  -  Loyally\n",
      "Denying  -  Denying\n",
      "catskilils  -  catskills\n",
      "Equlibrium  -  Equilibrium\n",
      "TRAIN - Epoch: 7  - Step: 7000  - Cost: 3.9405348  - CER: 0.13586591  - WER (step): 0.4375\n",
      "\n",
      "Train examples pred vs real:\n",
      "GOVERNESSES  -  GOVERNESSES\n",
      "BOASTED  -  BOASTED\n",
      "saturation  -  saturation\n",
      "SHAGgINg  -  SHAGGING\n",
      "NoTEPAPeR  -  Notepaper\n",
      "Csian  -  censurable\n",
      "CRIBBING  -  CRIBBING\n",
      "TRAIN - Epoch: 7  - Step: 7500  - Cost: 3.808667  - CER: 0.13109575  - WER (step): 0.3125\n",
      "\n",
      "Train examples pred vs real:\n",
      "PRINCIPLES  -  PRINCIPLES\n",
      "chlloroforms  -  chloroforms\n",
      "skimnier  -  skinnier\n",
      "dleferred  -  deferred\n",
      "murnikdier  -  murkier\n",
      "solicits  -  solicits\n",
      "CONTRAVENTION  -  CONTRAVENTION\n",
      "TRAIN - Epoch: 7  - Step: 8000  - Cost: 3.9027636  - CER: 0.13269821  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "disclaims  -  disclaims\n",
      "ROOTs  -  ROOTS\n",
      "POMEGRANATES  -  POMEGRANATES\n",
      "Inequaliies  -  Inequalities\n",
      "womanlier  -  womanlier\n",
      "DEAR  -  SODDY\n",
      "Aoopig  -  Hooping\n",
      "TRAIN - Epoch: 7  - Step: 8500  - Cost: 3.817919  - CER: 0.13408868  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "Shiner  -  Shiner\n",
      "Mothbaled  -  Mothballed\n",
      "vitreous  -  vitreous\n",
      "DHLECTICS  -  DIALECTICS\n",
      "BUCKSAWS  -  BUCKSAWS\n",
      "gainers  -  gainers\n",
      "trtangular  -  triangular\n",
      "TRAIN - Epoch: 7  - Step: 9000  - Cost: 3.883566  - CER: 0.13359202  - WER (step): 0.5\n",
      "\n",
      "Train examples pred vs real:\n",
      "garnishment  -  garnishment\n",
      "OUTSIZE  -  OUTSIZE\n",
      "DUCKBILLS  -  DUCKBILLS\n",
      "Egomania  -  Egomania\n",
      "ADUNCT  -  ADJUNCT\n",
      "Pariodontal  -  Periodontal\n",
      "NSTRUCTIONS  -  INSTRUCTIONS\n",
      "Testing epoch: 7\n",
      "TEST - Cost: 5.795496  - CER: 0.17465673  - WER: 0.5029296875\n",
      "\n",
      "Test examples pred vs real:\n",
      "Engravers  -  Engravers\n",
      "urbana  -  urbana\n",
      "Invoice  -  Invoice\n",
      "opulently  -  opulently\n",
      "boutmentalization  -  departmentalization\n",
      "PRCVE  -  PROVE\n",
      "HUTHORITY  -  authority\n",
      "faye  -  faye\n",
      "UNSTOP  -  UNSTOP\n",
      "sharpens  -  sharpens\n",
      "Model saved in file: /home/ubuntu/data/oxford_syntetic_text/models/test03/model-7\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "gpu_options = tf.GPUOptions(allow_growth = False)\n",
    "with tf.Session(graph=graph, config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True)) as sess:\n",
    "\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(experiment_dir, 'train'), graph=graph)\n",
    "    test_writer = tf.summary.FileWriter(os.path.join(experiment_dir, 'test'))\n",
    "\n",
    "\n",
    "    # Initialize vars if dont exist previous checkpoints. \n",
    "    ckpt = tf.train.get_checkpoint_state(experiment_dir)\n",
    "    if ckpt == None:\n",
    "        # Initialize vars\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('vars initialized!')\n",
    "        epoch_ini = 1\n",
    "    else:\n",
    "        # Load last model\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        if os.path.basename(ckpt.model_checkpoint_path).split('-')[-1] == 'best_model':\n",
    "            epoch_ini = 1\n",
    "        else:\n",
    "            epoch_ini = int(os.path.basename(ckpt.model_checkpoint_path).split('-')[-1]) + 1\n",
    "        print('model loaded: %s', ckpt.model_checkpoint_path)\n",
    "\n",
    "\n",
    "\n",
    "    # Compute for num_epochs.\n",
    "    if 1:\n",
    "        cost_val_l = []\n",
    "        cer_val_l = []\n",
    "        wer_val_l = []\n",
    "        continue_training = True\n",
    "        epoch = epoch_ini\n",
    "        while (epoch < num_epochs and continue_training):\n",
    "\n",
    "            # Train phase\n",
    "            print('Training epoch:', epoch)\n",
    "            train_step(epoch, decoder_dict)\n",
    "\n",
    "            # Test phase\n",
    "            print('Testing epoch:', epoch)\n",
    "            cost_val, cer_val, wer_val = eval_step(decoder_dict)\n",
    "            cost_val_l += [cost_val]\n",
    "\n",
    "                \n",
    "            # Check accuracy improventents and stop training\n",
    "            if len(cost_val_l) > 10:\n",
    "                print('cost val %s', cost_val_l)\n",
    "                if np.min(cost_val_l) < np.min(cost_val_l[-10]):\n",
    "                    continue_training = False\n",
    "                    print('STOPING TRAINING')\n",
    "                \n",
    "            #Save model\n",
    "            save_path = saver.save(sess, os.path.join(experiment_dir, 'model'), global_step=epoch)\n",
    "            print(\"Model saved in file: %s\" % save_path)  \n",
    "\n",
    "           \n",
    "            epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf18]",
   "language": "python",
   "name": "conda-env-tf18-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "283px",
    "left": "1076px",
    "right": "20px",
    "top": "73px",
    "width": "345px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
