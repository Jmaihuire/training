{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained word embeddings in a Keras model\n",
    "\n",
    "\n",
    "Based on https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),\n",
    "                                shuffle=True, random_state=42)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),\n",
    "                               shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'alt.atheism',\n",
       " 1: 'comp.graphics',\n",
       " 2: 'comp.os.ms-windows.misc',\n",
       " 3: 'comp.sys.ibm.pc.hardware',\n",
       " 4: 'comp.sys.mac.hardware',\n",
       " 5: 'comp.windows.x',\n",
       " 6: 'misc.forsale',\n",
       " 7: 'rec.autos',\n",
       " 8: 'rec.motorcycles',\n",
       " 9: 'rec.sport.baseball',\n",
       " 10: 'rec.sport.hockey',\n",
       " 11: 'sci.crypt',\n",
       " 12: 'sci.electronics',\n",
       " 13: 'sci.med',\n",
       " 14: 'sci.space',\n",
       " 15: 'soc.religion.christian',\n",
       " 16: 'talk.politics.guns',\n",
       " 17: 'talk.politics.mideast',\n",
       " 18: 'talk.politics.misc',\n",
       " 19: 'talk.religion.misc'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = data_train.data\n",
    "labels = data_train.target\n",
    "labels_index = {}\n",
    "for i,l in enumerate(data_train.target_names):\n",
    "    labels_index[i] = l\n",
    "    \n",
    "labels_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/tf12/lib/python3.6/site-packages/keras/preprocessing/text.py:139: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105372 unique tokens.\n",
      "Shape of data tensor: (11314, 1000)\n",
      "Shape of label tensor: (11314, 20)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/home/jorge/data/text'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(DATA_DIR, 'glove.6B/glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a 1D convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         10537300  \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 10,784,616\n",
      "Trainable params: 247,316\n",
      "Non-trainable params: 10,537,300\n",
      "_________________________________________________________________\n",
      "Train on 9052 samples, validate on 2262 samples\n",
      "Epoch 1/50\n",
      "9052/9052 [==============================] - 1s - loss: 2.9026 - acc: 0.0949 - val_loss: 2.5609 - val_acc: 0.1667\n",
      "Epoch 2/50\n",
      "9052/9052 [==============================] - 1s - loss: 2.3365 - acc: 0.2154 - val_loss: 2.0781 - val_acc: 0.2754\n",
      "Epoch 3/50\n",
      "9052/9052 [==============================] - 1s - loss: 1.8558 - acc: 0.3452 - val_loss: 1.8000 - val_acc: 0.3660\n",
      "Epoch 4/50\n",
      "9052/9052 [==============================] - 1s - loss: 1.5351 - acc: 0.4651 - val_loss: 1.5959 - val_acc: 0.4664\n",
      "Epoch 5/50\n",
      "9052/9052 [==============================] - 1s - loss: 1.3099 - acc: 0.5461 - val_loss: 1.4959 - val_acc: 0.5093\n",
      "Epoch 6/50\n",
      "9052/9052 [==============================] - 1s - loss: 1.1301 - acc: 0.6075 - val_loss: 1.4695 - val_acc: 0.5332\n",
      "Epoch 7/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.9741 - acc: 0.6607 - val_loss: 1.5017 - val_acc: 0.5442\n",
      "Epoch 8/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.8302 - acc: 0.7077 - val_loss: 1.6032 - val_acc: 0.5526\n",
      "Epoch 9/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.6903 - acc: 0.7586 - val_loss: 1.6759 - val_acc: 0.5592\n",
      "Epoch 10/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.5701 - acc: 0.7990 - val_loss: 1.8551 - val_acc: 0.5597\n",
      "Epoch 11/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.4754 - acc: 0.8312 - val_loss: 2.1731 - val_acc: 0.5539\n",
      "Epoch 12/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.4150 - acc: 0.8541 - val_loss: 2.3961 - val_acc: 0.5615\n",
      "Epoch 13/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.3623 - acc: 0.8814 - val_loss: 2.6773 - val_acc: 0.5588\n",
      "Epoch 14/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.3372 - acc: 0.8900 - val_loss: 2.7061 - val_acc: 0.5623\n",
      "Epoch 15/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.3177 - acc: 0.8956 - val_loss: 2.9758 - val_acc: 0.5513\n",
      "Epoch 16/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.3358 - acc: 0.8883 - val_loss: 2.8447 - val_acc: 0.5504\n",
      "Epoch 17/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.3498 - acc: 0.8886 - val_loss: 2.8347 - val_acc: 0.5544\n",
      "Epoch 18/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.3396 - acc: 0.8911 - val_loss: 2.8171 - val_acc: 0.5513\n",
      "Epoch 19/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.2860 - acc: 0.9102 - val_loss: 3.0459 - val_acc: 0.5539\n",
      "Epoch 20/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.2638 - acc: 0.9143 - val_loss: 3.0292 - val_acc: 0.5522\n",
      "Epoch 21/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.2467 - acc: 0.9223 - val_loss: 3.3793 - val_acc: 0.5517\n",
      "Epoch 22/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.2407 - acc: 0.9222 - val_loss: 3.3935 - val_acc: 0.5579\n",
      "Epoch 23/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.2421 - acc: 0.9222 - val_loss: 3.4062 - val_acc: 0.5535\n",
      "Epoch 24/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.2362 - acc: 0.9233 - val_loss: 3.3569 - val_acc: 0.5447\n",
      "Epoch 25/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.2225 - acc: 0.9319 - val_loss: 3.3845 - val_acc: 0.5681\n",
      "Epoch 26/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.2136 - acc: 0.9322 - val_loss: 3.5193 - val_acc: 0.5654\n",
      "Epoch 27/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1983 - acc: 0.9371 - val_loss: 3.5416 - val_acc: 0.5522\n",
      "Epoch 28/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1846 - acc: 0.9419 - val_loss: 3.3554 - val_acc: 0.5650\n",
      "Epoch 29/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1745 - acc: 0.9474 - val_loss: 3.4919 - val_acc: 0.5707\n",
      "Epoch 30/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1530 - acc: 0.9518 - val_loss: 3.6556 - val_acc: 0.5663\n",
      "Epoch 31/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1479 - acc: 0.9535 - val_loss: 3.8163 - val_acc: 0.5601\n",
      "Epoch 32/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1449 - acc: 0.9534 - val_loss: 3.7456 - val_acc: 0.5694\n",
      "Epoch 33/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1364 - acc: 0.9570 - val_loss: 3.8401 - val_acc: 0.5663\n",
      "Epoch 34/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1183 - acc: 0.9643 - val_loss: 4.0214 - val_acc: 0.5619\n",
      "Epoch 35/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1239 - acc: 0.9611 - val_loss: 4.0244 - val_acc: 0.5645\n",
      "Epoch 36/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1255 - acc: 0.9618 - val_loss: 3.9801 - val_acc: 0.5619\n",
      "Epoch 37/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1155 - acc: 0.9638 - val_loss: 4.1707 - val_acc: 0.5690\n",
      "Epoch 38/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1112 - acc: 0.9656 - val_loss: 4.1792 - val_acc: 0.5650\n",
      "Epoch 39/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1135 - acc: 0.9644 - val_loss: 4.3631 - val_acc: 0.5592\n",
      "Epoch 40/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1187 - acc: 0.9637 - val_loss: 4.4191 - val_acc: 0.5637\n",
      "Epoch 41/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1188 - acc: 0.9631 - val_loss: 4.2304 - val_acc: 0.5729\n",
      "Epoch 42/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1344 - acc: 0.9599 - val_loss: 4.1120 - val_acc: 0.5707\n",
      "Epoch 43/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1258 - acc: 0.9637 - val_loss: 4.0578 - val_acc: 0.5659\n",
      "Epoch 44/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1142 - acc: 0.9634 - val_loss: 4.1595 - val_acc: 0.5672\n",
      "Epoch 45/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1151 - acc: 0.9620 - val_loss: 4.0692 - val_acc: 0.5681\n",
      "Epoch 46/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1137 - acc: 0.9660 - val_loss: 4.3480 - val_acc: 0.5712\n",
      "Epoch 47/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1182 - acc: 0.9643 - val_loss: 4.2420 - val_acc: 0.5712\n",
      "Epoch 48/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1099 - acc: 0.9661 - val_loss: 4.2597 - val_acc: 0.5796\n",
      "Epoch 49/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.1044 - acc: 0.9674 - val_loss: 4.2330 - val_acc: 0.5756\n",
      "Epoch 50/50\n",
      "9052/9052 [==============================] - 1s - loss: 0.0964 - acc: 0.9704 - val_loss: 4.2597 - val_acc: 0.5769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5577677dd8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()\n",
    "\n",
    "sgd_optimizer = SGD(lr=0.01, momentum=0.99, decay=0.001, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd_optimizer,\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf12]",
   "language": "python",
   "name": "conda-env-tf12-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
